{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training Framework for PLC-Compatible LSTM Models  \n",
        "\n",
        "This module is part of the research project:  \n",
        "\n",
        "**_“Towards AI-Based Anomaly Detection at the Edge:  \n",
        "Evaluating Real-Time Cyber Defense in Programmable Logic Controllers”_**  \n",
        "\n",
        "The implementation defines a **deterministic training workflow** for LSTM-based anomaly detection models designed for deployment in **Programmable Logic Controllers (PLCs)**.  \n",
        "It ensures **reproducible experiments** and **consistent parameter management** between the training and export stages.  \n",
        "\n",
        "All random generators (`torch`, `numpy`, `random`) are seeded for full determinism.  \n",
        "The dataset is preprocessed through **feature selection, normalization, and time-windowed batching** for sequence learning.  \n",
        "\n",
        "The training process uses an **autoencoder-based LSTM** that reconstructs the last timestep of each input window.  \n",
        "The system includes **early stopping**, **calibration-based thresholding**, and **automatic checkpoint saving** per epoch.\n",
        "\n",
        "---\n",
        "\n",
        "## User Guide — Training Configuration and Execution  \n",
        "\n",
        "Only minimal configuration is required before training.  \n",
        "The training script automatically creates the model directory and saves all logs, checkpoints, and normalization references.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Configure Model Parameters  \n",
        "\n",
        "### Step 1: Configure Model Parameters  \n",
        "\n",
        "Before running the script, set these constants according to the trained model you want to export:\n",
        "\n",
        "```python\n",
        "MODEL_NAME = \"LSTM_SWaT\"\n",
        "MODEL_VERSION = \"v1\"\n",
        "HIDDEN_SIZE = 8\n",
        "SEQUENCE_LENGTH = 10\n",
        "BEST_EPOCH = 2\n",
        "```\n",
        "\n",
        "The base paths are defined dynamically in the script to ensure consistent folder structure:\n",
        "\n",
        "```python\n",
        "BASE_DIR = Path.cwd()\n",
        "PROJECT_DIR = BASE_DIR.parent\n",
        "DATASET_DIR = PROJECT_DIR / \"dataset\"\n",
        "```\n",
        "\n",
        "This logic automatically links the working directory to the project’s parent folder and the dataset location.  \n",
        "These paths are **only templates** — users may **freely modify** them to match their own project structure.  \n",
        "The important point is that the script always builds file references **relative to a defined project root**,  \n",
        "so all models, datasets, and export files remain organized and traceable.\n",
        "\n",
        "When execution begins, the program constructs the model export directory:\n",
        "\n",
        "```\n",
        "MODEL_DIR = /Models/HS8_IW10_v1/\n",
        "```\n",
        "\n",
        "- If this folder **does not exist**, it is **automatically created**.  \n",
        "- If it **already exists**, the user is prompted to confirm overwriting.  \n",
        "  - Selecting **“Yes”** allows file replacement.  \n",
        "  - Selecting **“No”** cancels execution and safely shuts down the notebook kernel.\n",
        "\n",
        "This setup gives users flexibility in organizing their project while ensuring safe and reproducible operation.\n",
        "\n",
        "\n",
        "\n",
        "### Step 2: Run Training  \n",
        "\n",
        "Run the notebook manually (avoid “Run All”).  \n",
        "Confirm overwrite if the target model directory already exists.  \n",
        "The script will automatically:\n",
        "- Load and preprocess the dataset  \n",
        "- Split it into train/calibration/test sets (60/20/20)  \n",
        "- Normalize all features using Z-score  \n",
        "- Train the LSTM autoencoder  \n",
        "- Save checkpoints, normalization data, and logs  \n",
        "\n",
        "No additional manual actions are required during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Check Results  \n",
        "\n",
        "After completion, verify that the following files exist in the model folder:\n",
        "The exact filenames depend on the user-defined configuration parameters\n",
        "(e.g., model name, hidden size, input window, version, and epoch).\n",
        "\n",
        "Example output files:\n",
        "\n",
        "```\n",
        "/Models/HS8_IW10_v1/\n",
        "│\n",
        "├── LSTM_SWaT_HS8_IW10_v1_Epoch_01.pt\n",
        "├── LSTM_SWaT_HS8_IW10_v1_Epoch_02.pt\n",
        "├── LSTM_SWaT_HS8_IW10_v1_featureNormRef_.csv\n",
        "├── LSTM_SWaT_HS8_IW10_v1_Learning_LOG.csv\n",
        "└── loss_plots.png  (optional)\n",
        "```\n",
        "\n",
        "\n",
        "## Legal Notice\n",
        "\n",
        "- Author: Anonymous  \n",
        "- Website: ---  \n",
        "- Contact: Anonymous  \n",
        "- License: Creative Commons BY-NC 4.0  \n",
        "- Version: v1.0.1 \n",
        "- Copyright: © 2025-2026\n",
        "\n",
        "This software is for educational and research purposes only or other non-commercial use.  \n",
        "Use in an industrial environment is forbidden.  \n",
        "The author must be credited in all use cases.  \n",
        "Commercial use requires written permission.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Important Notice\n",
        "\n",
        "**Do NOT use Run All**.\n",
        "\n",
        "This notebook requires an explicit user confirmation step. Run cells manually after confirmation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvzfHt3aDAEW",
        "outputId": "fcd43da2-7acf-4ecf-9585-e59798f7ed8b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import os, threading\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython import get_ipython\n",
        "import random\n",
        "import csv\n",
        "BASE_DIR = Path.cwd()\n",
        "PROJECT_DIR = BASE_DIR.parent\n",
        "DATASET_DIR = PROJECT_DIR / \"dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBWlImIGFsbH",
        "outputId": "cbc1c957-2bdb-4270-8531-ea6e1e390932"
      },
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 8\n",
        "SEQUENCE_LENGTH = 10\n",
        "MODEL_NAME = \"LSTM_SWaT\"\n",
        "MODEL_VERSION = \"v1\"\n",
        "NUM_EPOCHS = 1\n",
        "EARLY_STOPPING_PATIENCE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fLmio0FGf3I"
      },
      "outputs": [],
      "source": [
        "MODEL_FILENAME = f\"{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}.pt\"\n",
        "MODEL_DIR = f\"{PROJECT_DIR}/Models/HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}\"\n",
        "NORM_REFERENCE_FILENAME = f\"{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}_featureNormRef_.csv\"\n",
        "LEARN_DATA_FILENAME = \"SWaT_Dataset_Normal_v0.csv\"\n",
        "\n",
        "DROPOUT = 0.0\n",
        "NUM_LAYERS = 1\n",
        "BATCH_SIZE = 32 \n",
        "THRESHOLD_PERCENTILE = 92\n",
        "LEARNING_RATE = 0.001\n",
        "STRIDE = 1\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USER_CONFIRMED_OVERWRITE = False\n",
        "out = widgets.Output()\n",
        "if os.path.exists(MODEL_DIR):\n",
        "    print(f\"[WARNING] Directory already exists at: {MODEL_DIR}\")\n",
        "    print(\"Press YES to continue or NO to stop execution.\")\n",
        "\n",
        "    yes_button = widgets.Button(description=\"Yes\", button_style=\"danger\")\n",
        "    no_button  = widgets.Button(description=\"No\", button_style=\"success\")\n",
        "\n",
        "    def on_yes_clicked(b):\n",
        "        global USER_CONFIRMED_OVERWRITE\n",
        "        USER_CONFIRMED_OVERWRITE = True\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print(\"[WARNING] Existing files in this directory may be OVERWRITTEN!\")\n",
        "\n",
        "    def on_no_clicked(b):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print(\"[ABORTED] User cancelled execution.\")\n",
        "        get_ipython().kernel.do_shutdown(restart=False)\n",
        "\n",
        "    yes_button.on_click(on_yes_clicked)\n",
        "    no_button.on_click(on_no_clicked)\n",
        "\n",
        "    display(widgets.HBox([yes_button, no_button]))\n",
        "    display(out)\n",
        "\n",
        "else:\n",
        "    USER_CONFIRMED_OVERWRITE = True\n",
        "    os.makedirs(MODEL_DIR)\n",
        "    print(f\"[INFO] Directory created at: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(MODEL_DIR) and not USER_CONFIRMED_OVERWRITE:\n",
        "    print(\"[ABORTED] No user confirmation received. Stopping execution.\")\n",
        "    get_ipython().kernel.do_shutdown(restart=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HhJW8iC-XtQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6pj9JLWZJjK",
        "outputId": "0efb8b98-cea0-4e55-9dca-d5804342bc33"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load and Inspect Data ---\n",
        "df = pd.read_csv(os.path.join(DATASET_DIR, LEARN_DATA_FILENAME), encoding='utf-8-sig', header=1)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# --- 2. Model candidate features: numeric columns ---\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "numeric_columns = numeric_df.columns.tolist()\n",
        "\n",
        "# --- 3. Non-numeric columns (meta, timestamp, or labels) ---\n",
        "non_numeric_columns = [col for col in df.columns if col not in numeric_columns]\n",
        "\n",
        "# --- 4. Exclude constant (uninformative) numeric features ---\n",
        "min_row = numeric_df.min()\n",
        "max_row = numeric_df.max()\n",
        "status_row = np.where((max_row - min_row) == 0, 'CONST', 'VALID')\n",
        "transposed_df = pd.DataFrame(\n",
        "    [min_row.values, max_row.values, status_row],\n",
        "    columns=min_row.index,\n",
        "    index=['Min', 'Max', 'Status']\n",
        ")\n",
        "features_for_model = transposed_df.columns[transposed_df.loc['Status'] == 'VALID'].tolist()\n",
        "constant_numeric_columns = transposed_df.columns[transposed_df.loc['Status'] == 'CONST'].tolist()\n",
        "\n",
        "print(f\"Model candidate input features: {len(numeric_columns)}\")\n",
        "print(numeric_columns)\n",
        "print(f\"\\nDiscarded non-numeric columns: {len(non_numeric_columns)}\")\n",
        "print(non_numeric_columns)\n",
        "print(f\"\\nDiscarded constant numeric columns: {len(constant_numeric_columns)}\")\n",
        "print(constant_numeric_columns)\n",
        "print(f\"\\nFinal model input features (non-constant): {len(features_for_model)}\")\n",
        "print(features_for_model)\n",
        "print(f\"\\nTotal samples before removing missing values: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvNaHccKsWW2",
        "outputId": "b6a24548-30c1-4e80-faad-50fa8b7a7328"
      },
      "outputs": [],
      "source": [
        "# --- 5. Remove samples with missing values in model input features ---\n",
        "missing_mask = df[features_for_model].isnull().any(axis=1)\n",
        "n_missing = missing_mask.sum()\n",
        "print(f\"Samples with missing input features (removed): {n_missing}\")\n",
        "df_clean = df[~missing_mask].copy()\n",
        "print(f\"Total samples after removing missing values: {len(df_clean)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhCqTRh4K1ti",
        "outputId": "b930dd8f-12f2-49a9-cdd3-b134302095d5"
      },
      "outputs": [],
      "source": [
        "# --- 6. Train / Calib(threshold) / Test Split (időrendi) ---\n",
        "n = len(df_clean)\n",
        "train_end = int(n * 0.6)\n",
        "calib_end = int(n * 0.8)\n",
        "\n",
        "train_df = df_clean.iloc[:train_end].copy()\n",
        "calib_df = df_clean.iloc[train_end:calib_end].copy()   \n",
        "test_df  = df_clean.iloc[calib_end:].copy()            \n",
        "\n",
        "# --- 7. Z-score normalization ---\n",
        "scaler = StandardScaler()\n",
        "train_df[features_for_model] = scaler.fit_transform(train_df[features_for_model])\n",
        "calib_df[features_for_model] = scaler.transform(calib_df[features_for_model])\n",
        "test_df[features_for_model]  = scaler.transform(test_df[features_for_model])\n",
        "\n",
        "# --- 8. Check normalization correctness \n",
        "mean_tolerance = 1e-2  \n",
        "std_tolerance = 1e-2   \n",
        "\n",
        "means = train_df[features_for_model].mean()\n",
        "stds = train_df[features_for_model].std()\n",
        "\n",
        "bad_means = means[np.abs(means) > mean_tolerance]\n",
        "bad_stds = stds[np.abs(stds - 1) > std_tolerance]\n",
        "\n",
        "if not bad_means.empty or not bad_stds.empty:\n",
        "    print(\"WARNING: Train features were not properly normalized.\")\n",
        "    if not bad_means.empty:\n",
        "        print(\"Features with mean significantly different from 0:\")\n",
        "        print(bad_means)\n",
        "    if not bad_stds.empty:\n",
        "        print(\"Features with std significantly different from 1:\")\n",
        "        print(bad_stds)\n",
        "else:\n",
        "    print(\"Z-score normalization check: All TRAIN features normalized successfully.\")\n",
        "    \n",
        "NUM_CLASSES = len(features_for_model)\n",
        "print(\"Model input/output dim (NUM_CLASSES):\", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5B7qNnQK1qu"
      },
      "outputs": [],
      "source": [
        "# --- 9. Dataset and DataLoader Preparation ---\n",
        "class TempDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, features, stride=1):\n",
        "        self.data = data[features].values.astype('float32')\n",
        "        self.seq_len = seq_len\n",
        "        self.stride = stride\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data) - self.seq_len) // self.stride + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.stride\n",
        "        end = start + self.seq_len\n",
        "        seq = self.data[start:end, :]\n",
        "        return torch.from_numpy(seq)\n",
        "\n",
        "train_dataset = TempDataset(train_df, SEQUENCE_LENGTH, features_for_model, stride=STRIDE)\n",
        "calib_dataset = TempDataset(calib_df, SEQUENCE_LENGTH, features_for_model, stride=STRIDE)\n",
        "test_dataset  = TempDataset(test_df,  SEQUENCE_LENGTH, features_for_model, stride=STRIDE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "calib_loader = DataLoader(calib_dataset, batch_size=1, shuffle=False)  # early stop + threshold\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False)  # érintetlen mérés\n",
        "\n",
        "# --- 10. Model Definition ---\n",
        "class TempLSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, sequence_length):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "        self.sequence_length = sequence_length  # csak tárolás\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        y = self.fc(out[:, -1, :])  # (B, F)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4azWDqPFhym"
      },
      "source": [
        "###Train and create the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fxZc4FaEK1oC",
        "outputId": "00488351-b665-4580-92a0-6386e463f2fe"
      },
      "outputs": [],
      "source": [
        "# --- 11. Training ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TempLSTMAutoencoder(\n",
        "    input_size=NUM_CLASSES,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    sequence_length=SEQUENCE_LENGTH\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
        "best_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "train_loss_history = []\n",
        "calib_loss_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # MODEL TRAINING PHASE\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for inputs in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        target = inputs[:, -1, :]      \n",
        "        outputs = model(inputs)        \n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # --- CALIBRATION/VALIDATION PHASE/EARLY STOP ---\n",
        "    model.eval()\n",
        "    total_calib_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs in calib_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            target = inputs[:, -1, :]\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, target)\n",
        "            total_calib_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    avg_calib_loss = total_calib_loss / len(calib_dataset)\n",
        "    calib_loss_history.append(avg_calib_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.6f} | Calib Loss: {avg_calib_loss:.6f}\")\n",
        "\n",
        "    # --- SAVE MODEL FOR EACH EPOCH ---\n",
        "    model_path = f\"{MODEL_DIR}/{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}_Epoch_{epoch+1:02d}.pt\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # --- EARLY STOPPING (based on CALIB loss) ---\n",
        "    if avg_calib_loss < best_loss:\n",
        "        best_loss = avg_calib_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= early_stopping_patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# --- 12. Loss Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_loss_history, label='Train Loss', marker='o')\n",
        "plt.plot(calib_loss_history, label='Calib Loss', marker='s')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.title(f'Train vs Calib Loss per Epoch ({MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 13. Calculate THRESHOLD on CALIB dataset ---\n",
        "calib_loader = DataLoader(\n",
        "    calib_dataset,\n",
        "    batch_size=256,   # adjust if needed\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")\n",
        "model.eval()\n",
        "mse_calib = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs in calib_loader:\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "\n",
        "        targets = inputs[:, -1, :]     # (B, F)\n",
        "        outputs = model(inputs)        # (B, F)\n",
        "\n",
        "        mse = ((targets - outputs) ** 2).mean(dim=1)  # (B,)\n",
        "        mse_calib.append(mse)\n",
        "\n",
        "mse_calib = torch.cat(mse_calib).cpu().numpy()\n",
        "pred_threshold = np.percentile(mse_calib, THRESHOLD_PERCENTILE)\n",
        "threshold_percentile = THRESHOLD_PERCENTILE\n",
        "pred_threshold = np.percentile(mse_calib, threshold_percentile)\n",
        "print(f\"Prediction threshold (MSE, {threshold_percentile} percentile) FROM CALIB: {pred_threshold:.6f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(mse_calib, bins=100, alpha=0.7)\n",
        "plt.axvline(pred_threshold, color='red', linestyle='--',\n",
        "            label=f'Threshold ({threshold_percentile} percentile)')\n",
        "plt.title('CALIB Set Reconstruction Error Distribution')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Sample Count')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 14. Save LEARNING LOG to CSV (loss history + key training params) ---\n",
        "learning_log_path = os.path.join(\n",
        "    MODEL_DIR,\n",
        "    f\"{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}_Learning_LOG.csv\"\n",
        ")\n",
        "\n",
        "loss_header = [\"Epoch\", \"Train_Loss\", \"Calib_Loss\"]\n",
        "loss_rows = list(zip(\n",
        "    range(1, len(train_loss_history) + 1),\n",
        "    train_loss_history,\n",
        "    calib_loss_history\n",
        "))\n",
        "\n",
        "param_header = [\n",
        "    \"LEARNING_RATE\",\n",
        "    \"EARLY_STOPPING_PATIENCE\",\n",
        "    \"NUM_LAYERS\",\n",
        "    \"HIDDEN_SIZE\",\n",
        "    \"SEQUENCE_LENGTH\",\n",
        "    \"BATCH_SIZE\",\n",
        "    \"DROPOUT\",\n",
        "    \"SEED\",\n",
        "    \"THRESHOLD_PERCENTILE\",\n",
        "    \"BEST_CALIB_LOSS\",\n",
        "    \"EPOCHS_RAN\",\n",
        "    \"DEVICE\",\n",
        "    \"STRIDE\",\n",
        "]\n",
        "param_values = [\n",
        "    LEARNING_RATE,\n",
        "    EARLY_STOPPING_PATIENCE,\n",
        "    NUM_LAYERS,\n",
        "    HIDDEN_SIZE,\n",
        "    SEQUENCE_LENGTH,\n",
        "    BATCH_SIZE,\n",
        "    DROPOUT,\n",
        "    SEED,\n",
        "    THRESHOLD_PERCENTILE,\n",
        "    best_loss,\n",
        "    len(train_loss_history),\n",
        "    str(device),\n",
        "    STRIDE,\n",
        "]\n",
        "with open(learning_log_path, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    #params section\n",
        "    w.writerow(param_header)\n",
        "    w.writerow(param_values)\n",
        "    # Loss section\n",
        "    w.writerow([])\n",
        "    w.writerow(loss_header)\n",
        "    w.writerows(loss_rows)\n",
        "\n",
        "# --- 15. Save model and normalization parameters with logs ---\n",
        "dummy_input = torch.randn(1, SEQUENCE_LENGTH, NUM_CLASSES).to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _ = model(dummy_input)\n",
        "\n",
        "# Save normalization parameters\n",
        "scaler_df = pd.DataFrame({'mean': scaler.mean_, 'std': scaler.scale_}, index=features_for_model)\n",
        "scaler_df.to_csv(os.path.join(MODEL_DIR, NORM_REFERENCE_FILENAME))\n",
        "\n",
        "# Save full model\n",
        "torch.save(model, os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
        "print(f\"Model, and normalization parameters saved: {model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
