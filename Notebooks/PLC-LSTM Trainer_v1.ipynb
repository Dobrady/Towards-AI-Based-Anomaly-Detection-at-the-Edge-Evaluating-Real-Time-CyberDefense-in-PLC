{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvzfHt3aDAEW",
        "outputId": "fcd43da2-7acf-4ecf-9585-e59798f7ed8b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython import get_ipython\n",
        "\n",
        "#import joblib\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBWlImIGFsbH",
        "outputId": "cbc1c957-2bdb-4270-8531-ea6e1e390932"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path.cwd()\n",
        "PROJECT_DIR = BASE_DIR.parent\n",
        "DATASET_DIR = PROJECT_DIR / \"dataset\"\n",
        "\n",
        "\n",
        "HIDDEN_SIZE = 8\n",
        "SEQUENCE_LENGTH = 10\n",
        "MODEL_NAME = \"LSTM_SWaT\"\n",
        "MODEL_VERSION = \"v1\"\n",
        "NUM_EPOCHS = 2\n",
        "EARLY_STOPPING_PATIENCE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fLmio0FGf3I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARNING] Directory already exists at: c:\\Repos\\Towards-AI-Based-Anomaly-Detection-at-the-Edge-Evaluating-Real-Time-CyberDefense-in-PLC/Models/HS8_IW10_v1\n",
            "CONTINUE?\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27e18b61c96d4c8993bb6856cdb44781",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Button(button_style='danger', description='Yes', style=ButtonStyle()), Button(button_style='suc…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02c5a4020ec24a5b9913b3ebc4afc8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "MODEL_FILENAME = f\"{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}.pt\"\n",
        "MODEL_DIR = f\"{PROJECT_DIR}/Models/HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}\"\n",
        "NORM_REFERENCE_FILENAME = f\"featureNormRef_{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}.csv\"\n",
        "LEARN_DATA_FILENAME = \"SWaT_Dataset_Normal_v0.csv\"\n",
        "\n",
        "DROPOUT = 0.0\n",
        "NUM_LAYERS = 1\n",
        "BATCH_SIZE = 32 \n",
        "THRESHOLD_PERCENTILE = 92\n",
        "LEARNING_RATE = 0.001\n",
        "SEED = 42\n",
        "\n",
        "# --- 1. Hard-stop execution if MODEL_DIR exists and user cancels ---\n",
        "out = widgets.Output()  # <-- KÜLÖN output widget\n",
        "\n",
        "if os.path.exists(MODEL_DIR):\n",
        "    print(f\"[WARNING] Directory already exists at: {MODEL_DIR}\\nCONTINUE?\")\n",
        "\n",
        "    yes_button = widgets.Button(description=\"Yes\", button_style='danger')\n",
        "    no_button = widgets.Button(description=\"No, STOP execution\", button_style='success')\n",
        "\n",
        "    def on_yes_clicked(b):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print(\"[WARNING] Existing files in this directory may be OVERWRITTEN!\")\n",
        "          \n",
        "    def on_no_clicked(b):\n",
        "        with out:\n",
        "            out.clear_output()\n",
        "            print(\"[ABORTED] User cancelled execution. Kernel will shut down.\")\n",
        "        get_ipython().kernel.do_shutdown(restart=False)\n",
        "\n",
        "    yes_button.on_click(on_yes_clicked)\n",
        "    no_button.on_click(on_no_clicked)\n",
        "\n",
        "    display(widgets.HBox([yes_button, no_button]))\n",
        "    display(out)\n",
        "\n",
        "else:\n",
        "    os.makedirs(MODEL_DIR)\n",
        "    print(f\"[INFO] Directory created at:\\n{MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check existence and create if necessary\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "    print(f\"Directory created at:\\n{MODEL_DIR}\")\n",
        "else:\n",
        "    print(f\"DIRECTORY ALREADY EXIST AT:\\n{MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HhJW8iC-XtQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6pj9JLWZJjK",
        "outputId": "0efb8b98-cea0-4e55-9dca-d5804342bc33"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load and Inspect Data ---\n",
        "df = pd.read_csv(os.path.join(DATASET_DIR, LEARN_DATA_FILENAME), encoding='utf-8-sig', header=1)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# --- 2. Model candidate features: numeric columns ---\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "numeric_columns = numeric_df.columns.tolist()\n",
        "\n",
        "# --- 3. Non-numeric columns (meta, timestamp, or labels) ---\n",
        "non_numeric_columns = [col for col in df.columns if col not in numeric_columns]\n",
        "\n",
        "# --- 4. Exclude constant (uninformative) numeric features ---\n",
        "min_row = numeric_df.min()\n",
        "max_row = numeric_df.max()\n",
        "status_row = np.where((max_row - min_row) == 0, 'CONST', 'VALID')\n",
        "transposed_df = pd.DataFrame(\n",
        "    [min_row.values, max_row.values, status_row],\n",
        "    columns=min_row.index,\n",
        "    index=['Min', 'Max', 'Status']\n",
        ")\n",
        "features_for_model = transposed_df.columns[transposed_df.loc['Status'] == 'VALID'].tolist()\n",
        "constant_numeric_columns = transposed_df.columns[transposed_df.loc['Status'] == 'CONST'].tolist()\n",
        "\n",
        "print(f\"Model candidate input features: {len(numeric_columns)}\")\n",
        "print(numeric_columns)\n",
        "print(f\"\\nDiscarded non-numeric columns: {len(non_numeric_columns)}\")\n",
        "print(non_numeric_columns)\n",
        "print(f\"\\nDiscarded constant numeric columns: {len(constant_numeric_columns)}\")\n",
        "print(constant_numeric_columns)\n",
        "print(f\"\\nFinal model input features (non-constant): {len(features_for_model)}\")\n",
        "print(features_for_model)\n",
        "print(f\"\\nTotal samples before removing missing values: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvNaHccKsWW2",
        "outputId": "b6a24548-30c1-4e80-faad-50fa8b7a7328"
      },
      "outputs": [],
      "source": [
        "# --- 5. Remove samples with missing values in model input features ---\n",
        "missing_mask = df[features_for_model].isnull().any(axis=1)\n",
        "n_missing = missing_mask.sum()\n",
        "print(f\"Samples with missing input features (removed): {n_missing}\")\n",
        "df_clean = df[~missing_mask].copy()\n",
        "print(f\"Total samples after removing missing values: {len(df_clean)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhCqTRh4K1ti",
        "outputId": "b930dd8f-12f2-49a9-cdd3-b134302095d5"
      },
      "outputs": [],
      "source": [
        "# --- 6. Train / Calib(threshold) / Test Split (időrendi) ---\n",
        "n = len(df_clean)\n",
        "train_end = int(n * 0.6)\n",
        "calib_end = int(n * 0.8)\n",
        "\n",
        "train_df = df_clean.iloc[:train_end].copy()\n",
        "calib_df = df_clean.iloc[train_end:calib_end].copy()   \n",
        "test_df  = df_clean.iloc[calib_end:].copy()            \n",
        "\n",
        "# --- 7. Z-score normalization ---\n",
        "scaler = StandardScaler()\n",
        "train_df[features_for_model] = scaler.fit_transform(train_df[features_for_model])\n",
        "calib_df[features_for_model] = scaler.transform(calib_df[features_for_model])\n",
        "test_df[features_for_model]  = scaler.transform(test_df[features_for_model])\n",
        "\n",
        "# --- 8. Check normalization correctness \n",
        "mean_tolerance = 1e-2  \n",
        "std_tolerance = 1e-2   \n",
        "\n",
        "means = train_df[features_for_model].mean()\n",
        "stds = train_df[features_for_model].std()\n",
        "\n",
        "bad_means = means[np.abs(means) > mean_tolerance]\n",
        "bad_stds = stds[np.abs(stds - 1) > std_tolerance]\n",
        "\n",
        "if not bad_means.empty or not bad_stds.empty:\n",
        "    print(\"WARNING: Train features were not properly normalized.\")\n",
        "    if not bad_means.empty:\n",
        "        print(\"Features with mean significantly different from 0:\")\n",
        "        print(bad_means)\n",
        "    if not bad_stds.empty:\n",
        "        print(\"Features with std significantly different from 1:\")\n",
        "        print(bad_stds)\n",
        "else:\n",
        "    print(\"Z-score normalization check: All TRAIN features normalized successfully.\")\n",
        "    \n",
        "NUM_CLASSES = len(features_for_model)\n",
        "print(\"Model input/output dim (NUM_CLASSES):\", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5B7qNnQK1qu"
      },
      "outputs": [],
      "source": [
        "# --- 9. Dataset and DataLoader Preparation ---\n",
        "class TempDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, features, stride=1):\n",
        "        self.data = data[features].values.astype('float32')\n",
        "        self.seq_len = seq_len\n",
        "        self.stride = stride\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data) - self.seq_len) // self.stride + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.stride\n",
        "        end = start + self.seq_len\n",
        "        seq = self.data[start:end, :]\n",
        "        return torch.from_numpy(seq)\n",
        "\n",
        "train_dataset = TempDataset(train_df, SEQUENCE_LENGTH, features_for_model, stride=1)\n",
        "calib_dataset = TempDataset(calib_df, SEQUENCE_LENGTH, features_for_model, stride=1)\n",
        "test_dataset  = TempDataset(test_df,  SEQUENCE_LENGTH, features_for_model, stride=1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "calib_loader = DataLoader(calib_dataset, batch_size=1, shuffle=False)  # early stop + threshold\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=1, shuffle=False)  # érintetlen mérés\n",
        "\n",
        "# --- 10. Model Definition ---\n",
        "class TempLSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, sequence_length):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "        self.sequence_length = sequence_length  # csak tárolás\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        y = self.fc(out[:, -1, :])  # (B, F)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4azWDqPFhym"
      },
      "source": [
        "###Train and create the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fxZc4FaEK1oC",
        "outputId": "00488351-b665-4580-92a0-6386e463f2fe"
      },
      "outputs": [],
      "source": [
        "# --- 11. Training ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TempLSTMAutoencoder(\n",
        "    input_size=NUM_CLASSES,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    sequence_length=SEQUENCE_LENGTH\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
        "best_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "train_loss_history = []\n",
        "calib_loss_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # MODEL TRAINING PHASE\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for inputs in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        target = inputs[:, -1, :]      # (B,F)\n",
        "        outputs = model(inputs)        # (B,F)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # --- CALIBRATION/VALIDATION PHASE (EARLY STOP ITT) ---\n",
        "    model.eval()\n",
        "    total_calib_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs in calib_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            target = inputs[:, -1, :]\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, target)\n",
        "            total_calib_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    avg_calib_loss = total_calib_loss / len(calib_dataset)\n",
        "    calib_loss_history.append(avg_calib_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.6f} | Calib Loss: {avg_calib_loss:.6f}\")\n",
        "\n",
        "    # --- SAVE MODEL FOR EACH EPOCH ---\n",
        "    model_path = f\"{MODEL_DIR}/{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}_model_epoch_{epoch+1:02d}.pt\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # --- EARLY STOPPING (based on CALIB loss) ---\n",
        "    if avg_calib_loss < best_loss:\n",
        "        best_loss = avg_calib_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= early_stopping_patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# --- 12. Loss Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_loss_history, label='Train Loss', marker='o')\n",
        "plt.plot(calib_loss_history, label='Calib Loss', marker='s')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.title(f'Train vs Calib Loss per Epoch ({MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 13. Save loss history to CSV ---\n",
        "loss_df = pd.DataFrame({\n",
        "    'Epoch': list(range(1, len(train_loss_history) + 1)),\n",
        "    'Train_Loss': train_loss_history,\n",
        "    'Calib_Loss': calib_loss_history\n",
        "})\n",
        "loss_df.to_csv(f\"{MODEL_DIR}/{MODEL_NAME}_HS{HIDDEN_SIZE}_IW{SEQUENCE_LENGTH}_{MODEL_VERSION}_loss_history.csv\", index=False)\n",
        "\n",
        "# --- 14. Calculate THRESHOLD on CALIB dataset ---\n",
        "model.eval()\n",
        "mse_calib = []\n",
        "with torch.no_grad():\n",
        "    for inputs in calib_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = inputs[:, -1, :].cpu().numpy()   \n",
        "        outputs = model(inputs).cpu().numpy()      \n",
        "        mse_calib.append(((targets - outputs) ** 2).mean(axis=1))  \n",
        "\n",
        "mse_calib = np.concatenate(mse_calib, axis=0) \n",
        "print(f\"\\nCalib reconstruction error: mean MSE = {mse_calib.mean():.6f}, std = {mse_calib.std():.6f}\")\n",
        "\n",
        "threshold_percentile = THRESHOLD_PERCENTILE\n",
        "pred_threshold = np.percentile(mse_calib, threshold_percentile)\n",
        "print(f\"Prediction threshold (MSE, {threshold_percentile} percentile) FROM CALIB: {pred_threshold:.6f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(mse_calib, bins=100, alpha=0.7)\n",
        "plt.axvline(pred_threshold, color='red', linestyle='--',\n",
        "            label=f'Threshold ({threshold_percentile} percentile)')\n",
        "plt.title('CALIB Set Reconstruction Error Distribution')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Sample Count')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogMilwaFRNMG"
      },
      "source": [
        "### LSTM modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZSpj3I0To4T"
      },
      "outputs": [],
      "source": [
        "# --- 15. Confirm and Save Model and Normalization Parameters ---1\n",
        "def confirm_and_save_model(model, dummy_input):\n",
        "    def save_all():\n",
        "        model.eval()\n",
        "\n",
        "        # Save normalization parameters to CSV\n",
        "        scaler_df = pd.DataFrame(\n",
        "            {'mean': scaler.mean_, 'std': scaler.scale_},\n",
        "            index=features_for_model\n",
        "        )\n",
        "        scaler_df.to_csv(os.path.join(MODEL_DIR, NORM_REFERENCE_FILENAME))\n",
        "\n",
        "        # Save FULL PyTorch model (.pt)\n",
        "        torch.save(model, os.path.join(MODEL_DIR, MODEL_FILENAME))\n",
        "\n",
        "        print(\"Full PyTorch model (.pt) and normalization parameters saved. \")\n",
        "\n",
        "    # Check for existing PyTorch model file\n",
        "    if not os.path.exists(os.path.join(MODEL_DIR, MODEL_FILENAME)):\n",
        "        save_all()\n",
        "        return\n",
        "\n",
        "    print(f\"PyTorch model file already exists at: {os.path.join(MODEL_DIR, MODEL_FILENAME)}\")\n",
        "    yes_button = widgets.Button(description=\"Yes, overwrite\", button_style='danger')\n",
        "    no_button = widgets.Button(description=\"No\")\n",
        "\n",
        "    def on_yes_clicked(b):\n",
        "        clear_output()\n",
        "        save_all()\n",
        "\n",
        "    def on_no_clicked(b):\n",
        "        clear_output()\n",
        "        print(\"Model saving cancelled.\")\n",
        "\n",
        "    yes_button.on_click(on_yes_clicked)\n",
        "    no_button.on_click(on_no_clicked)\n",
        "    display(widgets.HBox([yes_button, no_button]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRMK8R7NV6aF",
        "outputId": "8842cb2f-f860-49ff-ebf8-832b0c72a8f6"
      },
      "outputs": [],
      "source": [
        "# --- 16. Trigger Model Saving with Dummy Input Verification ---\n",
        "dummy_input = torch.randn(1, SEQUENCE_LENGTH, NUM_CLASSES).to(device)\n",
        "confirm_and_save_model(model=model, dummy_input=dummy_input)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
