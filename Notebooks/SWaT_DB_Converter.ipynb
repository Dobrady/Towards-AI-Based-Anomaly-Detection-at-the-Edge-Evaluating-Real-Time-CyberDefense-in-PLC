{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWaT Dataset → Siemens PLC DB Converter\n",
    "\n",
    "Part of the research toolset for:\n",
    "\n",
    "**_“Towards AI-Based Anomaly Detection at the Edge:  \n",
    "Evaluating Real-Time Cyber Defense in Programmable Logic Controllers”_**\n",
    "\n",
    "---\n",
    "\n",
    "This tool converts labeled SWaT dataset `.csv` files into  \n",
    "**Siemens-compatible `DATA_BLOCK` (`.db`) files** for PLC testing and simulation.\n",
    "\n",
    "Each row is validated and normalized into `REAL`-typed values,  \n",
    "including input features and a binary label (`normal = 0.0`, `attack = 1.0`).\n",
    "\n",
    "The generated DB file contains:\n",
    "\n",
    "- A 2D `ARRAY[0..N, 0..M] OF REAL` structure  \n",
    "- Fully deterministic format, ready for Siemens TIA Portal  \n",
    "- Suitable for both testing and educational use\n",
    "\n",
    "---\n",
    "\n",
    "## User Guide\n",
    "\n",
    "This tool runs fully offline. Configuration is done via constants at the top of the script.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: File Setup\n",
    "\n",
    "```python\n",
    "SWAT_INPUT_DATASET_FILE_NAME = \"your_file.csv\"\n",
    "SWAT_OUTPUT_DB_FILE_NAME     = \"your_file.db\"\n",
    "```\n",
    "\n",
    "You may also change:\n",
    "- `BLOCK_NAME`, `VERSION`, or `VAR_NAME` if needed\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Run the Notebook\n",
    "\n",
    "Run **manually** (not with “Run All”). The converter will:\n",
    "\n",
    "- Validate the CSV structure  \n",
    "- Normalize numeric inputs  \n",
    "- Map labels (`normal` or `attack`)  \n",
    "- Generate the `.db` output\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Review Output\n",
    "\n",
    "After execution:\n",
    "\n",
    "- Output appears in the defined folder  \n",
    "- Overwrite must be confirmed if file exists  \n",
    "- You get summary statistics:\n",
    "\n",
    "```text\n",
    "[INFO] Total samples: 11545\n",
    "[INFO] Total columns: 41 (40 features + 1 label)\n",
    "[INFO] Label distribution:\n",
    "   - 0.0: 9471 samples (82.03%)\n",
    "   - 1.0: 2074 samples (17.97%)\n",
    "[INFO] Integer-to-float conversions: 3832\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Unknown labels or column mismatch will raise errors  \n",
    "- Integers are always converted (e.g., `250` → `250.0`)  \n",
    "- All output values are `REAL` (float) for PLC compatibility\n",
    "\n",
    "---\n",
    "\n",
    "## Legal Notice  \n",
    "\n",
    "- **Author:** Anonymous  \n",
    "- **Website:** ---  \n",
    "- **Contact:** later  \n",
    "- **License:** Creative Commons BY-NC 4.0  \n",
    "- **Version:** v1.0.1  \n",
    "- **Copyright:** © 2025–2026\n",
    "\n",
    "This software is intended exclusively for **educational and research purposes**, or other **non-commercial applications**.  \n",
    "Use in **industrial production environments** is prohibited.  \n",
    "The **author must be credited** in all derivative or redistributive works.  \n",
    "**Commercial use** requires **explicit written permission** from the author.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "BASE_DIR = Path.cwd()\n",
    "PROJECT_DIR = BASE_DIR.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Input folder (c:\\Repos\\Towards-AI-Based-Anomaly-Detection-at-the-Edge-Evaluating-Real-Time-CyberDefense-in-PLC\\dataset):\n",
      "   - Attack_v0_first_attack.csv\n",
      "   - db\n",
      "   - SWaT_Dataset_Normal_v0.csv\n",
      "[INFO] Output folder (c:\\Repos\\Towards-AI-Based-Anomaly-Detection-at-the-Edge-Evaluating-Real-Time-CyberDefense-in-PLC\\dataset\\db):\n",
      "   - Attack_v0_first_attack.db\n"
     ]
    }
   ],
   "source": [
    "# --- Input CSV folder\n",
    "SWAT_DATASET_DIR = PROJECT_DIR / \"dataset\"\n",
    "\n",
    "# --- Output DB folder\n",
    "OUTPUT_DB_DIR = PROJECT_DIR / \"dataset/db\"\n",
    "\n",
    "\n",
    "# --- List contents of input and output folders\n",
    "print(f\"[INFO] Input folder ({SWAT_DATASET_DIR}):\")\n",
    "for f in sorted(SWAT_DATASET_DIR.glob(\"*\")):\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "print(f\"[INFO] Output folder ({OUTPUT_DB_DIR}):\")\n",
    "for f in sorted(OUTPUT_DB_DIR.glob(\"*\")):\n",
    "    print(f\"   - {f.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output DB file will be created: c:\\Repos\\Towards-AI-Based-Anomaly-Detection-at-the-Edge-Evaluating-Real-Time-CyberDefense-in-PLC\\dataset\\db\\Attack_v0_first_attack.db\n"
     ]
    }
   ],
   "source": [
    "# --- Input CSV filename (source dataset)           \n",
    "SWAT_INPUT_DATASET_FILE_NAME = \"Attack_v0_first_attack.csv\"\n",
    "\n",
    "# --- Output DB filename (Siemens PLC format) \n",
    "SWAT_OUTPUT_DB_FILE_NAME     = \"Attack_v0_first_attack.db\"\n",
    "\n",
    "# ----- DB INTERNAL CONTENT  -----\n",
    "BLOCK_NAME   = \"Attack_v0_first_attack\"   # name of the DATA_BLOCK\n",
    "VERSION      = \"0.1\"                      # DB version\n",
    "VAR_NAME     = \"DATA\"                     # name of the 2D array\n",
    "\n",
    "REQUIRED_FEATURE_COUNT = 40 # Number of input features expected per row (excluding the target label)\n",
    "\n",
    "# --- Warn if output file already exists\n",
    "if (OUTPUT_DB_DIR / SWAT_OUTPUT_DB_FILE_NAME).exists():\n",
    "    print(f\"[WARNING] Output DB file already exists: {OUTPUT_DB_DIR / SWAT_OUTPUT_DB_FILE_NAME}\")\n",
    "else:\n",
    "    print(f\"[INFO] Output DB file will be created: {OUTPUT_DB_DIR / SWAT_OUTPUT_DB_FILE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FILE PATHS -----\n",
    "IN_FILE  = SWAT_DATASET_DIR / SWAT_INPUT_DATASET_FILE_NAME\n",
    "OUT_FILE = OUTPUT_DB_DIR / SWAT_OUTPUT_DB_FILE_NAME\n",
    "\n",
    "# ----- DB DIMENSIONS -----\n",
    "DATA_COLS = REQUIRED_FEATURE_COUNT          # number of data columns\n",
    "LABEL_COL_INDEX = REQUIRED_FEATURE_COUNT    # label column index\n",
    "TOTAL_COLS = REQUIRED_FEATURE_COUNT + 1     # total columns stored\n",
    "       \n",
    "# ----- DB INDEX OFFSETS -----       \n",
    "ROW_START = 0   # starting row index in DB\n",
    "COL_START = 0   # starting column index in DB\n",
    "\n",
    "# ----- CSV INPUT CONFIG -----\n",
    "CSV_DELIMITER        = \",\"          # delimiter of the CSV input file \n",
    "CSV_ENCODING         = \"utf-8\"      # encoding of the CSV input file\n",
    "SKIP_HEADER_LINES    = 1            # number of header lines to skip in the CSV input file\n",
    "\n",
    "# ----- STATS -----\n",
    "INTEGER_CONVERSION_COUNT = 0\n",
    "\n",
    "# ----- REGEXES -----\n",
    "_integer_re = re.compile(r\"^[+-]?\\d+$\")  \n",
    "_numeric_point_re = re.compile(r\"^[+-]?(?:\\d+(?:\\.\\d+)?|\\.\\d+)(?:[eE][+-]?\\d+)?$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize numeric input: trim, convert integers to float format ---\n",
    "def normalize_data_cell(cell: str) -> str:\n",
    "    global INTEGER_CONVERSION_COUNT\n",
    "    \n",
    "    c = cell.strip().strip(\",;\")\n",
    "    \n",
    "    if _integer_re.fullmatch(c):\n",
    "        INTEGER_CONVERSION_COUNT += 1 \n",
    "        c = f\"{c}.0\"\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "# --- Map raw label values to standard numeric form (\"normal\" → 0.0, \"attack\" → 1.0) ---\n",
    "def map_label_cell(cell: str) -> str:\n",
    "    raw = cell.strip().strip(\"\\\"'\")\n",
    "    if _numeric_point_re.fullmatch(raw):\n",
    "        return raw\n",
    "    \n",
    "    lowered = raw.casefold().strip()\n",
    "    \n",
    "    if lowered == \"normal\":\n",
    "        return \"0.0\"\n",
    "    \n",
    "    if lowered == \"attack\":\n",
    "        return \"1.0\"\n",
    "    \n",
    "    raise ValueError(f\"Unknown label value: '{cell}'\")\n",
    "\n",
    "\n",
    "# --- Read CSV rows: enforce column count, normalize values, and map labels ---\n",
    "def read_rows(path: Path) -> list[list[str]]:\n",
    "    rows: list[list[str]] = []\n",
    "    delimiter = \"\\t\" if CSV_DELIMITER in {\"\\\\t\", \"\\t\"} else CSV_DELIMITER\n",
    "\n",
    "    with path.open(\"r\", encoding=CSV_ENCODING, errors=\"ignore\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "\n",
    "        for _ in range(SKIP_HEADER_LINES):\n",
    "            next(reader, None)\n",
    "\n",
    "        for line_idx, cols in enumerate(reader, start=1 + SKIP_HEADER_LINES):\n",
    "            if not cols or all(c.strip() == \"\" for c in cols):\n",
    "                continue\n",
    "\n",
    "            if len(cols) < TOTAL_COLS:\n",
    "                raise ValueError(\n",
    "                    f\"{line_idx}. row: too few columns ({len(cols)} < {TOTAL_COLS})\"\n",
    "                )\n",
    "\n",
    "            if len(cols) > TOTAL_COLS:\n",
    "                raise ValueError(\n",
    "                    f\"{line_idx}. row: too many columns ({len(cols)} > {TOTAL_COLS})\"\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                data_cells = [normalize_data_cell(c) for c in cols[:DATA_COLS]]\n",
    "                label_cell = map_label_cell(cols[LABEL_COL_INDEX])\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Error in row {line_idx}: {e}\") from None\n",
    "\n",
    "            rows.append(data_cells + [label_cell])\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No valid data rows found after header.\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "# --- Generate PLC-compatible DB text from normalized data rows ---\n",
    "def build_db_text(rows: list[list[str]]) -> str:\n",
    "    n_rows = len(rows)\n",
    "    row_end = ROW_START + n_rows - 1\n",
    "    col_end = COL_START + TOTAL_COLS - 1  # 0..40\n",
    "\n",
    "    header = [\n",
    "        f'DATA_BLOCK \"{BLOCK_NAME}\"',\n",
    "        \"{ S7_Optimized_Access := 'TRUE' }\",\n",
    "        f\"VERSION : {VERSION}\",\n",
    "        \"NON_RETAIN\",\n",
    "        \"   VAR\",\n",
    "        f\"      {VAR_NAME} : ARRAY[{ROW_START}..{row_end}, {COL_START}..{col_end}] OF REAL;\",\n",
    "        \"   END_VAR\",\n",
    "        \"BEGIN\",\n",
    "    ]\n",
    "\n",
    "    body: list[str] = []\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, val in enumerate(row):\n",
    "            if val == \"\":\n",
    "                continue\n",
    "            body.append(f\"{VAR_NAME}[{i+ROW_START},{j+COL_START}] := {val};\")\n",
    "\n",
    "    footer = [\"END_DATA_BLOCK\"]\n",
    "    return \"\\n\".join(header + body + footer) + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    rows = read_rows(IN_FILE)\n",
    "    db_text = build_db_text(rows)\n",
    "\n",
    "    # ---- Basic statistics ----\n",
    "    num_rows = len(rows)\n",
    "    num_columns = TOTAL_COLS\n",
    "    label_values = [row[-1] for row in rows]\n",
    "\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(label_values)\n",
    "    label_stats = {label: f\"{(count / num_rows) * 100:.2f}%\" for label, count in label_counts.items()}\n",
    "\n",
    "    print(f\"[INFO] Total samples: {num_rows}\")\n",
    "    print(f\"[INFO] Total columns: {num_columns} ({DATA_COLS} features + 1 label)\")\n",
    "    print(f\"[INFO] Label distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"       - {label}: {count} samples ({label_stats[label]})\")\n",
    "    print(f\"[INFO] Integer-to-float conversions (e.g. 250 → 250.0): {INTEGER_CONVERSION_COUNT}\")\n",
    "\n",
    "\n",
    "    # ---- Output handling ----\n",
    "    out = widgets.Output()\n",
    "\n",
    "    if os.path.exists(OUT_FILE):\n",
    "        print(f\"[WARNING] File already exists at: {OUT_FILE}\")\n",
    "        print(\"Press YES to overwrite or NO to skip saving.\")\n",
    "\n",
    "        yes_button = widgets.Button(description=\"Yes\", button_style=\"danger\")\n",
    "        no_button  = widgets.Button(description=\"No\",  button_style=\"success\")\n",
    "\n",
    "        def _disable_buttons():\n",
    "            yes_button.disabled = True\n",
    "            no_button.disabled  = True\n",
    "\n",
    "        def on_yes_clicked(b):\n",
    "            _disable_buttons()\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                OUT_FILE.write_text(db_text, encoding=CSV_ENCODING)\n",
    "                print(f\"[INFO] DB file overwritten: {OUT_FILE}\")\n",
    "\n",
    "        def on_no_clicked(b):\n",
    "            _disable_buttons()\n",
    "            with out:\n",
    "                out.clear_output()\n",
    "                print(\"[INFO] DB file exists, not saved.\")\n",
    "\n",
    "        yes_button.on_click(on_yes_clicked)\n",
    "        no_button.on_click(on_no_clicked)\n",
    "\n",
    "        display(widgets.HBox([yes_button, no_button]))\n",
    "        display(out)\n",
    "\n",
    "    else:\n",
    "        OUT_FILE.write_text(db_text, encoding=CSV_ENCODING)\n",
    "        print(f\"[INFO] DB file created at: {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total samples: 2694\n",
      "[INFO] Total columns: 41 (40 features + 1 label)\n",
      "[INFO] Label distribution:\n",
      "       - 0.0: 1754 samples (65.11%)\n",
      "       - 1.0: 940 samples (34.89%)\n",
      "[INFO] Integer-to-float conversions (e.g. 250 → 250.0): 42347\n",
      "[INFO] DB file created at: c:\\Repos\\Towards-AI-Based-Anomaly-Detection-at-the-Edge-Evaluating-Real-Time-CyberDefense-in-PLC\\dataset\\db\\Attack_v0_first_attack.db\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
